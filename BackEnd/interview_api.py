"""
é¢è©¦å•é¡Œç”Ÿæˆ API
"""
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import httpx
import json
import os
import asyncio

router = APIRouter()

# åˆ¤æ–·é‹è¡Œç’°å¢ƒ
ENVIRONMENT = os.getenv('ENVIRONMENT', 'development')
IS_PRODUCTION = ENVIRONMENT == 'production'

# LLM é…ç½® - æ ¹æ“šç’°å¢ƒè‡ªå‹•é¸æ“‡
if IS_PRODUCTION:
    # ç”Ÿç”¢ç’°å¢ƒï¼šä½¿ç”¨ AkashML
    LLM_CONFIG = {
        'api_key': os.getenv('LLM_API_KEY', 'akml-RTl88SQKMDZFX2c43QslImWLO7DNUdee'),
        'api_host': os.getenv('LLM_API_HOST', 'https://api.akashml.com'),
        'model': os.getenv('LLM_MODEL', 'deepseek-ai/DeepSeek-V3.1'),
        'endpoint': os.getenv('LLM_API_HOST', 'https://api.akashml.com') + '/v1/chat/completions'
    }
    print("ğŸŒ é¢è©¦ API ä½¿ç”¨ AkashML")
else:
    # é–‹ç™¼ç’°å¢ƒï¼šä½¿ç”¨ SiliconFlow
    LLM_CONFIG = {
        'api_key': os.getenv('LLM_API_KEY', 'sk-xmwxrtsxgsjwuyeceydoyuopezzlqresdjyvlzrbbjeejiff'),
        'api_host': os.getenv('LLM_API_HOST', 'https://api.siliconflow.cn'),
        'model': os.getenv('LLM_MODEL', 'deepseek-ai/DeepSeek-V3'),
        'endpoint': os.getenv('LLM_API_HOST', 'https://api.siliconflow.cn') + '/v1/chat/completions'
    }
    print("ğŸŒ é¢è©¦ API ä½¿ç”¨ SiliconFlow")

class InterviewRequest(BaseModel):
    candidates: List[Dict[str, Any]]
    conversation_history: Optional[List[Dict[str, str]]] = []

class InterviewResponse(BaseModel):
    questions: str
    conversation_id: str

@router.post("/api/generate-interview-questions", response_model=InterviewResponse)
async def generate_interview_questions(request: InterviewRequest):
    """ç”Ÿæˆé¢è©¦å•é¡Œ"""
    try:
        # è¨ºæ–·è³‡è¨Š
        print(f"\n{'='*60}")
        print(f"ğŸ” é¢è©¦å•é¡Œç”Ÿæˆè«‹æ±‚")
        print(f"{'='*60}")
        print(f"å€™é¸äººæ•¸é‡: {len(request.candidates)}")
        print(f"LLM API ç«¯é»: {LLM_CONFIG['endpoint']}")
        print(f"LLM æ¨¡å‹: {LLM_CONFIG['model']}")
        print(f"API Key (å‰10å­—): {LLM_CONFIG['api_key'][:10]}...")
        print(f"{'='*60}\n")
        # æ§‹å»ºå€™é¸äººä¿¡æ¯æ‘˜è¦
        candidates_summary = []
        for candidate in request.candidates:
            trait_results = candidate.get('trait_results', {})
            
            # æå–é«˜åˆ†ç‰¹è³ª
            high_traits = []
            for trait_key, trait_data in trait_results.items():
                if isinstance(trait_data, dict):
                    score = trait_data.get('score', 0)
                    chinese_name = trait_data.get('chinese_name', trait_key)
                    if score >= 70:
                        high_traits.append(f"{chinese_name}({score:.0f}åˆ†)")
            
            candidates_summary.append({
                'name': candidate.get('name', 'æœªçŸ¥'),
                'position': candidate.get('position', 'æœªçŸ¥'),
                'company': candidate.get('company', 'æœªçŸ¥'),
                'high_traits': high_traits[:5]  # åªå–å‰5å€‹
            })
        
        # æ§‹å»º prompt
        if not request.conversation_history:
            # é¦–æ¬¡ç”Ÿæˆ
            prompt = f"""ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ HR é¢è©¦å®˜ã€‚è«‹æ ¹æ“šä»¥ä¸‹å€™é¸äººçš„ç‰¹è³ªæ¸¬è©•çµæœï¼Œç‚ºæ¯ä½å€™é¸äººç”Ÿæˆ 3-5 å€‹é‡å°æ€§çš„é¢è©¦å•é¡Œã€‚

å€™é¸äººä¿¡æ¯ï¼š
"""
            for i, candidate in enumerate(candidates_summary, 1):
                prompt += f"\n{i}. {candidate['name']}"
                if candidate['position']:
                    prompt += f" - {candidate['position']}"
                if candidate['company']:
                    prompt += f" ({candidate['company']})"
                prompt += f"\n   å„ªå‹¢ç‰¹è³ªï¼š{', '.join(candidate['high_traits'])}\n"
            
            prompt += """
è«‹ç‚ºæ¯ä½å€™é¸äººç”Ÿæˆé¢è©¦å•é¡Œï¼Œè¦æ±‚ï¼š
1. å•é¡Œæ‡‰è©²é‡å°å€™é¸äººçš„å„ªå‹¢ç‰¹è³ªè¨­è¨ˆ
2. å•é¡Œæ‡‰è©²æ˜¯é–‹æ”¾å¼çš„ï¼Œèƒ½å¤ æ·±å…¥äº†è§£å€™é¸äººçš„èƒ½åŠ›
3. å•é¡Œæ‡‰è©²å…·é«”ã€å¯¦ç”¨ï¼Œä¾¿æ–¼åœ¨é¢è©¦ä¸­ä½¿ç”¨
4. æ¯å€‹å•é¡Œå¾Œé¢ç°¡è¦èªªæ˜è€ƒå¯Ÿç›®çš„

è«‹ä»¥æ¸…æ™°çš„æ ¼å¼è¼¸å‡ºï¼Œæ¯ä½å€™é¸äººçš„å•é¡Œåˆ†é–‹åˆ—å‡ºã€‚

**é‡è¦ï¼šè«‹å‹™å¿…ä½¿ç”¨ç¹é«”ä¸­æ–‡å›è¦†ï¼Œä¸è¦ä½¿ç”¨ç°¡é«”ä¸­æ–‡ã€‚**"""
        else:
            # å¤šè¼ªå°è©±
            prompt = request.conversation_history[-1]['content']
        
        # èª¿ç”¨ LLM
        messages = [
            {
                'role': 'system',
                'content': 'ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ HR é¢è©¦å®˜ï¼Œæ“…é•·æ ¹æ“šå€™é¸äººçš„ç‰¹è³ªè¨­è¨ˆé‡å°æ€§çš„é¢è©¦å•é¡Œã€‚è«‹å‹™å¿…ä½¿ç”¨ç¹é«”ä¸­æ–‡å›è¦†ã€‚'
            }
        ]
        
        # æ·»åŠ å°è©±æ­·å²
        if request.conversation_history:
            for msg in request.conversation_history:
                messages.append({
                    'role': msg['role'],
                    'content': msg['content']
                })
        else:
            messages.append({
                'role': 'user',
                'content': prompt
            })
        
        # é‡è©¦æ©Ÿåˆ¶
        max_retries = 3
        retry_delay = 2
        
        for attempt in range(max_retries):
            try:
                print(f"ğŸ“¡ å˜—è©¦èª¿ç”¨ LLM API (ç¬¬ {attempt + 1} æ¬¡)")
                
                async with httpx.AsyncClient(timeout=60.0) as client:
                    request_data = {
                        'model': LLM_CONFIG['model'],
                        'messages': messages,
                        'temperature': 0.7,
                        'max_tokens': 2000
                    }
                    
                    print(f"è«‹æ±‚è³‡æ–™å¤§å°: {len(str(request_data))} å­—å…ƒ")
                    
                    response = await client.post(
                        LLM_CONFIG['endpoint'],
                        headers={
                            'Content-Type': 'application/json',
                            'Authorization': f'Bearer {LLM_CONFIG["api_key"]}'
                        },
                        json=request_data
                    )
                    
                    print(f"ğŸ“¥ æ”¶åˆ°å›æ‡‰: ç‹€æ…‹ç¢¼ {response.status_code}")
                    
                    if response.status_code == 200:
                        result = response.json()
                        questions = result['choices'][0]['message']['content']
                        
                        return InterviewResponse(
                            questions=questions,
                            conversation_id=str(hash(questions))
                        )
                    elif response.status_code == 503 and attempt < max_retries - 1:
                        # 503 éŒ¯èª¤ä¸”é‚„æœ‰é‡è©¦æ©Ÿæœƒï¼Œç­‰å¾…å¾Œé‡è©¦
                        print(f"âš ï¸ LLM API 503 éŒ¯èª¤ï¼Œ{retry_delay} ç§’å¾Œé‡è©¦ (å˜—è©¦ {attempt + 1}/{max_retries})")
                        await asyncio.sleep(retry_delay)
                        retry_delay *= 2  # æŒ‡æ•¸é€€é¿
                        continue
                    else:
                        error_detail = f"LLM API éŒ¯èª¤: {response.status_code}"
                        try:
                            error_body = response.json()
                            error_detail += f" - {error_body}"
                            print(f"âŒ API éŒ¯èª¤è©³æƒ…: {error_body}")
                        except:
                            try:
                                error_text = response.text
                                print(f"âŒ API éŒ¯èª¤æ–‡æœ¬: {error_text[:500]}")
                                error_detail += f" - {error_text[:200]}"
                            except:
                                pass
                        raise HTTPException(status_code=500, detail=error_detail)
            except httpx.TimeoutException:
                if attempt < max_retries - 1:
                    print(f"âš ï¸ LLM API è¶…æ™‚ï¼Œ{retry_delay} ç§’å¾Œé‡è©¦ (å˜—è©¦ {attempt + 1}/{max_retries})")
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 2
                    continue
                else:
                    raise HTTPException(status_code=504, detail="LLM API è«‹æ±‚è¶…æ™‚")
            except httpx.RequestError as e:
                if attempt < max_retries - 1:
                    print(f"âš ï¸ LLM API é€£ç·šéŒ¯èª¤: {e}ï¼Œ{retry_delay} ç§’å¾Œé‡è©¦")
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 2
                    continue
                else:
                    raise HTTPException(status_code=503, detail=f"LLM API é€£ç·šå¤±æ•—: {str(e)}")
    
    except Exception as e:
        print(f"ç”Ÿæˆé¢è©¦å•é¡ŒéŒ¯èª¤: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))
